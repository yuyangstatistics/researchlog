<!DOCTYPE html>
<html lang="en"><head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="twitter:card" content="summary_large_image" /><!-- Begin Jekyll SEO tag v2.6.1 -->
<title>Digest – Recent advances in natural language inference: A survey of benchmarks, resources, and approaches | Yu Yang’s Research Log</title>
<meta name="generator" content="Jekyll v4.1.1" />
<meta property="og:title" content="Digest – Recent advances in natural language inference: A survey of benchmarks, resources, and approaches" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="Recent advances in natural language inference: A survey of benchmarks, resources, and approaches (Storks et al., 2019)." />
<meta property="og:description" content="Recent advances in natural language inference: A survey of benchmarks, resources, and approaches (Storks et al., 2019)." />
<link rel="canonical" href="https://yuyangyy.com/researchlog/nli/2021/01/20/recent-advances-in-nli.html" />
<meta property="og:url" content="https://yuyangyy.com/researchlog/nli/2021/01/20/recent-advances-in-nli.html" />
<meta property="og:site_name" content="Yu Yang’s Research Log" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2021-01-20T15:34:00-06:00" />
<script type="application/ld+json">
{"url":"https://yuyangyy.com/researchlog/nli/2021/01/20/recent-advances-in-nli.html","@type":"BlogPosting","headline":"Digest – Recent advances in natural language inference: A survey of benchmarks, resources, and approaches","dateModified":"2021-01-20T15:34:00-06:00","datePublished":"2021-01-20T15:34:00-06:00","mainEntityOfPage":{"@type":"WebPage","@id":"https://yuyangyy.com/researchlog/nli/2021/01/20/recent-advances-in-nli.html"},"description":"Recent advances in natural language inference: A survey of benchmarks, resources, and approaches (Storks et al., 2019).","@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->
<link rel="stylesheet" href="/researchlog/assets/css/style.css"><link type="application/atom+xml" rel="alternate" href="https://yuyangyy.com/researchlog/feed.xml" title="Yu Yang's Research Log" /><link rel="shortcut icon" type="image/x-icon" href="/researchlog/images/favicon.ico"><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/Primer/15.2.0/primer.css" integrity="sha512-xTz2ys4coGAOz8vuV1NcQBkgVmKhsSEtjbqyMJbBHRplFuvKIUo6xhLHpAyPt9mfR6twHJgn9OgVLuqOvjeBhg==" crossorigin="anonymous" />
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.14.0/css/all.min.css" integrity="sha512-1PKOgIY59xJ8Co8+NE6FZ+LOAZKjy+KY8iq0G4B3CyeY6wYHN3yt9PW0XpSriVlkMXe40PTKnXrLnZ9+fkDaog==" crossorigin="anonymous" />

<script>
function wrap_img(fn) {
    if (document.attachEvent ? document.readyState === "complete" : document.readyState !== "loading") {
        var elements = document.querySelectorAll(".post img");
        Array.prototype.forEach.call(elements, function(el, i) {
            if (el.getAttribute("title") && (el.className != "emoji")) {
                const caption = document.createElement('figcaption');
                var node = document.createTextNode(el.getAttribute("title"));
                caption.appendChild(node);
                const wrapper = document.createElement('figure');
                wrapper.className = 'image';
                el.parentNode.insertBefore(wrapper, el);
                el.parentNode.removeChild(el);
                wrapper.appendChild(el);
                wrapper.appendChild(caption);
            }
        });
    } else { document.addEventListener('DOMContentLoaded', fn); }
}
window.onload = wrap_img;
</script>

<script>
    document.addEventListener("DOMContentLoaded", function(){
    // add link icon to anchor tags
    var elem = document.querySelectorAll(".anchor-link")
    elem.forEach(e => (e.innerHTML = '<i class="fas fa-link fa-xs"></i>'));
    });
</script>
</head>
<body><header class="site-header">

  <div class="wrapper"><a class="site-title" rel="author" href="/researchlog/">Yu Yang&#39;s Research Log</a><nav class="site-nav">
        <input type="checkbox" id="nav-trigger" class="nav-trigger" />
        <label for="nav-trigger">
          <span class="menu-icon">
            <svg viewBox="0 0 18 15" width="18px" height="15px">
              <path d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.032C17.335,0,18,0.665,18,1.484L18,1.484z M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.032C17.335,6.031,18,6.696,18,7.516L18,7.516z M18,13.516C18,14.335,17.335,15,16.516,15H1.484 C0.665,15,0,14.335,0,13.516l0,0c0-0.82,0.665-1.483,1.484-1.483h15.032C17.335,12.031,18,12.695,18,13.516L18,13.516z"/>
            </svg>
          </span>
        </label>

        <div class="trigger"><a class="page-link" href="/researchlog/about/">About Me</a><a class="page-link" href="/researchlog/search/">Search</a><a class="page-link" href="/researchlog/categories/">Tags</a></div>
      </nav></div>
</header>
<main class="page-content" aria-label="Content">
      <div class="wrapper">
        <article class="post h-entry" itemscope itemtype="http://schema.org/BlogPosting">

  <header class="post-header">
    <h1 class="post-title p-name" itemprop="name headline">Digest -- Recent advances in natural language inference: A survey of benchmarks, resources, and approaches</h1><p class="post-meta post-meta-title"><time class="dt-published" datetime="2021-01-20T15:34:00-06:00" itemprop="datePublished">
        Jan 20, 2021
      </time>
       • <span class="read-time" title="Estimated read time">
    
    
      4 min read
    
</span></p>

    
      <p class="category-tags"><i class="fas fa-tags category-tags-icon"></i></i> 
      
        <a class="category-tags-link" href="/researchlog/categories/#NLI">NLI</a>
        
      
      </p>
    

    </header>

  <div class="post-content e-content" itemprop="articleBody">
    <p><a href="https://arxiv.org/abs/1904.01172">Recent advances in natural language inference: A survey of benchmarks, resources, and approaches</a>  <a class="citation" href="#storks2019recent">(Storks et al., 2019)</a>.</p>

<h2 id="summary">Summary</h2>
<p>This is a really good review paper in NLI. It mainly covers language understanding tasks and benchmarks where we need to <strong>use some external knowledge or advanced reasoning beyond linguistic context</strong>. The idea that we can better guide researchers to focus on truly understand the reasoning by designing smarter benchmarks is inspiring. This paper gives an overview of existing benchmarks and what problems they are trying to solve, as well as existing knowledge resources and inference approaches. It also provides examples from the benchmark datasets, which can give beginners some basic idea. It can serve as a pretty good reference for resources looking up. Several issues raised in this paper are worth attention, such as the unexplainability of recent approaches and the statistical biases found in benchmark datasets.</p>

<h3 id="benchmarks-and-tasks">Benchmarks and Tasks</h3>
<p>Five major tasks require external knowledge and complex reasoning: reference resolution, question answering, textual entailment, plausible inference, and intuitive psychology. It seems to me that the difference between textual entailment and plausible inference is that text entailment judges the correctness of hypothese and focuses on reasoning, while plausible inference finds the event that is the most likely to happen according to commonse knowledge.</p>

<p>The authors also call attention for the superficial correlation biases in the datasets, for example, the gender bias. Mutual information method <a class="citation" href="#gururangan2018annotation">(Gururangan et al., 2018)</a> and adversarial filtering process <a class="citation" href="#zellers2018swag">(Zellers et al., 2018)</a> may be helpful for such biases.</p>

<h3 id="knowledge-resources">Knowledge Resources</h3>
<p>Linguistic knowledge includes annotated corpora, frame semantics resources, lexical resources, and pre-trained semantic vectors.</p>

<p>Common and commonsense knowledge resources are mostly in the form of knowledge base and knowledge graph. To clarify, common knowledge refers to well-known facts about the world that are often explicitly stated, while commonsense knowledge, on the other hand, is considered obvious to most humans, and not likely to be explicitly stated <a class="citation" href="#cambria2011isanette">(Cambria et al., 2011)</a>.</p>

<h3 id="learning-and-inference-approaches">Learning and Inference Approaches</h3>
<p>Three main neural approaches are brought up: attention mechanism, memory augmentation, and contextual models and representations.</p>

<p>It points out that attention mechanism works well mainly on capturing the alignment between an input and an output, and capturing long-term dependencies. One thing to note is some RNN models with attention will perform worse since there is no such alignment. This reminds us to keep in mind what a structure is actually learning before stacking them altogether.</p>

<p>Memory augmentation methods, such as memory networks, are new to me and requires further reading.</p>

<p>One interesting point about using external knowledge is mentioned: <a class="citation" href="#mihaylov2018can">(Mihaylov et al., 2018)</a> find that their adding of facts from ConceptNet causes distraction which reduces performance, suggesting that the technique for selecting the appropriate relations is important to reduce distraction.</p>

<h3 id="future-directions">Future Directions</h3>
<p>The directions are mostly for designing datasets, still, I get some motivations.</p>

<p>Despite the good performance of current models, we don’t know whether or not they are actually performing reasoning. The authors think the benchmarks should differentiate between types of reasoning and take that into evaluations.</p>

<blockquote>
  <p>A competence-centric evaluation, while important for pushing the state of the art, can also lead to a less productive path if not treated carefully.</p>
</blockquote>

<p>The authors suggest that we put more attention on a good understanding of model behaviors (anything insightful? what is the model actually learning?), computational efficiency, and generalization ability (inference on new tasks with minimal training).</p>

<h2 id="questions">Questions</h2>
<ol>
  <li>How to use common or commonsense knowledge in creating a benchmark dataset?</li>
  <li>What are the existing types of reasoning?</li>
</ol>

<h2 id="related-topics-to-read">Related topics to read</h2>
<ul>
  <li>mutual information</li>
  <li>ConceptNet</li>
  <li>BERT is indeed learning and exploiting statistical biases in certain benchmarks <a class="citation" href="#niven2019probing">(Niven &amp; Kao, 2019)</a> .</li>
  <li>distributional representation of words <a class="citation" href="#ferrone2020symbolic">(Ferrone &amp; Zanzotto, 2020)</a>.</li>
  <li>types of reasoning <a class="citation" href="#davis2015commonsense">(Davis &amp; Marcus, 2015)</a></li>
</ul>

<h2 id="references">References</h2>

<!-- <a class="citation" href="#"></a> -->

<ol class="bibliography"><li><span id="storks2019recent">Storks, S., Gao, Q., &amp; Chai, J. Y. (2019). Recent advances in natural language inference: A survey of benchmarks, resources, and approaches. <i>ArXiv Preprint ArXiv:1904.01172</i>.</span></li>
<li><span id="gururangan2018annotation">Gururangan, S., Swayamdipta, S., Levy, O., Schwartz, R., Bowman, S., &amp; Smith, N. A. (2018). Annotation Artifacts in Natural Language Inference Data. <i>Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 2 (Short Papers)</i>, 107–112.</span></li>
<li><span id="zellers2018swag">Zellers, R., Bisk, Y., Schwartz, R., &amp; Choi, Y. (2018). SWAG: A Large-Scale Adversarial Dataset for Grounded Commonsense Inference. <i>Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</i>, 93–104.</span></li>
<li><span id="cambria2011isanette">Cambria, E., Song, Y., Wang, H., &amp; Hussain, A. (2011). Isanette: A common and common sense knowledge base for opinion mining. <i>2011 IEEE 11th International Conference on Data Mining Workshops</i>, 315–322.</span></li>
<li><span id="mihaylov2018can">Mihaylov, T., Clark, P., Khot, T., &amp; Sabharwal, A. (2018). Can a Suit of Armor Conduct Electricity? A New Dataset for Open Book Question Answering. <i>Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</i>, 2381–2391.</span></li>
<li><span id="niven2019probing">Niven, T., &amp; Kao, H.-Y. (2019). Probing Neural Network Comprehension of Natural Language Arguments. <i>Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</i>, 4658–4664.</span></li>
<li><span id="ferrone2020symbolic">Ferrone, L., &amp; Zanzotto, F. M. (2020). Symbolic, distributed, and distributional representations for natural language processing in the era of deep learning: A survey. <i>Frontiers in Robotics and AI</i>, <i>6</i>, 153.</span></li>
<li><span id="davis2015commonsense">Davis, E., &amp; Marcus, G. (2015). Commonsense reasoning and commonsense knowledge in artificial intelligence. <i>Communications of the ACM</i>, <i>58</i>(9), 92–103.</span></li></ol>

  </div><!-- from https://github.com/utterance/utterances -->
<script src="https://utteranc.es/client.js"
        repo="yuyangstatistics/researchlog"
        issue-term="title"
        label="blogpost-comment"
        theme="github-light"
        crossorigin="anonymous"
        async>
</script><a class="u-url" href="/researchlog/nli/2021/01/20/recent-advances-in-nli.html" hidden></a>
</article>
      </div>
    </main><footer class="site-footer h-card">
  <data class="u-url" href="/researchlog/"></data>

  <div class="wrapper">

    <div class="footer-col-wrapper">
      <div class="footer-col">
        <p class="feed-subscribe">
          <a href="/researchlog/feed.xml">
            <svg class="svg-icon orange">
              <use xlink:href="/researchlog/assets/minima-social-icons.svg#rss"></use>
            </svg><span>Subscribe</span>
          </a>
        </p>
      </div>
      <div class="footer-col">
        <p>Statistics Ph.D. student/ Yu Yang. My blog about paper reading, research ideas, and code.</p>
      </div>
    </div>

    <div class="social-links"><ul class="social-media-list"><li><a rel="me" href="https://github.com/yuyangstatistics" title="yuyangstatistics"><svg class="svg-icon grey"><use xlink:href="/researchlog/assets/minima-social-icons.svg#github"></use></svg></a></li><li><a rel="me" href="https://twitter.com/yuyangstat" title="yuyangstat"><svg class="svg-icon grey"><use xlink:href="/researchlog/assets/minima-social-icons.svg#twitter"></use></svg></a></li></ul>
</div>

  </div>

</footer>
</body>

</html>
