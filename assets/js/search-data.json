{
  
    
        "post0": {
            "title": "Digest -- Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer",
            "content": "Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer (Raffel et al., 2020). . Summary . This paper is more engineering-oriented than method-oriented in my opinion. It doesn’t propose new model arcthitectures or training techniques. Yet the contribution of this paper is tremendous. With the goal of investigating the exact contribution of various architectures, training objectives, techniques, and training datasets on transfer learning in NLP, the authors perform a series of systematic experiments and show us the optimal and promising strategies to consider empirically. Afterwards, they combine all of their findings and propose a pre-trained model T5 and the dataset C4. Another highlight is that they use a unified text-to-text framework in the study, namely, transform every NLP tasks into a text-to-text. . The logic to use a unified framework, to my understanding, is that if it can achieve good results across a wide range of tasks, then it may be implied that the model has indeed learned something universal in natural languages. . T5: Text-to-Text Transfer Transformer . . The architecture in the framework is encoder-decoder, so every task should be transformed in an input-output format, where both are text. To help the model identify the specific task to perform, the task name is appended at the beginning of the input. The excellence of T5 comes from the combination of optimal strategies with respect to multiple aspects, including encoder-decoder architecture, corrupting span denoising objective, C4 pre-training data set, Multi-task pre-training + fine-tuning on downstream tasks, and scaling in terms of model sizes and training time. . Experiment Results . Architectures: three types are compared: encoder-decoder, decoder-only language models, decoder-only prefix language models. Controlling the number of parameters, encoder-decoder is the best across all downstream tasks. . Unsupervised Objectives: the flow chart below shows their exploration and the table illustrates these objectives by an example (the captions are include details about the experiment). They choose the corrupted span denoising objective in that the computation cost can be reduced while maintaining comparable performances as the baseline. . . . A detailed illustration of the objective is as below. . Pre-training Data Set: they find that pre-training on in-domain unlabeld data can improve performance on downstream tasks, for example, SQuAD, which is based on Wikipedia. Also, with regard to the data set size, they find that some amount of repetition of the pre-training data might not be harmful. . . Training Strategy: they experiment three fine-tuning strategies and find that fine-tuning all parameters is the best. They also consider multi-task learning, where they mix the unsupervised task with the other tasks and explore three strategies of settting the proportion of data coming from each task. It turns out first pre-training on the unsupervised task and then fine-tuning on the downstream tasks outperforms the multi-task training strategies. They then combine multi-task learning with fine-tuning and compare different strategies as below. Since multi-task pre-training + fine-tuning has similar performance as unsupervise pre-training + fine-tuning, and that it enables us to monitor the “downstream” performance for the entire duration of traininng, instead of just fine-tuning, the authors use this strategy in their final T5 model. . . Scaling: they focus on addressing this question: “You were just given 4× more compute. How should you use it?” They find that in general, increasing the model size resulted in an additional bump in performance compared to solely increasing the training time or batch size. And ensembling provides an orthogonal and effective means of improving performance through scale. . . In the end, they put together all of their findings, combine the optimal settings, and come up with T5, a pre-trained model with state-of-art performances on many leaderboards. . Related topics to read . Multi-task Learning: An Overview of Multi-Task Learning in Deep Neural Networks | . References . Raffel, C., Shazeer, N., Roberts, A., Lee, K., Narang, S., Matena, M., Zhou, Y., Li, W., &amp; Liu, P. J. (2020). Exploring the limits of transfer learning with a unified text-to-text transformer. Journal of Machine Learning Research, 21(140), 1–67. |",
            "url": "https://yuyangyy.com/researchlog/nlp/2021/01/25/t5.html",
            "relUrl": "/nlp/2021/01/25/t5.html",
            "date": " • Jan 25, 2021"
        }
        
    
  
    
        ,"post1": {
            "title": "Digest -- Deep Learning: A Critical Appraisal",
            "content": "Deep Learning: A Critical Appraisal (Marcus, 2018). . Summary . Gary Marcus is a professor in Psychology and Neural Science at New York University. He is also the author of (Davis &amp; Marcus, 2015). In this paper, he reflected on the current issues of Deep Learning and emphasized that DL is not a universal tool and we need to know what it is and is not and good for. . Limits on the scope of deep learning . Generalization comes in two flavors, interpolation between known examples, and extrapolation, which requires going beyond a space of known training examples. . | Deep learning works best for classification problems with sufficient data samples in stable domains where examples are mapped in a constant way onto a limited set of categories. | Deep learning currently lacks a mechanism for learning abstractions through explicit, verbal definition. | Many adversarial studies show that deep learning solutions are often extremely superficial. | Language has a hierarchical structure, which deep learning so far has no natural way to deal with. This is a good point and might be helpful to model design. And a related question: can PCE learn the hierarchical structure? | Other issues: Open-ended inference, transparency, integration with prior knowledge, causation vs. correlation (even statistical techniques have limits in this.), an assumption of stable domains, not credible and no sound guarantees about performance. | . Future Direction . Integrate symbolic systems, which excels at inference and abstraction, with deep learning, which excels at perceptual classification. . Related topics to read . symbolic systems, representations of abstract relationships. | convolution, translational invariance | hierarchical structure in natural language. A paper showing the incapability of RNN: (Lake &amp; Baroni, 2018). | . References . Marcus, G. (2018). Deep learning: A critical appraisal. ArXiv Preprint ArXiv:1801.00631. | Davis, E., &amp; Marcus, G. (2015). Commonsense reasoning and commonsense knowledge in artificial intelligence. Communications of the ACM, 58(9), 92–103. | Lake, B., &amp; Baroni, M. (2018). Generalization without systematicity: On the compositional skills of sequence-to-sequence recurrent networks. International Conference on Machine Learning, 2873–2882. |",
            "url": "https://yuyangyy.com/researchlog/dl/2021/01/22/a-critical-appraisal-on-dl.html",
            "relUrl": "/dl/2021/01/22/a-critical-appraisal-on-dl.html",
            "date": " • Jan 22, 2021"
        }
        
    
  
    
        ,"post2": {
            "title": "Digest -- Commonsense reasoning and commonsense knowledge in artificial intelligence",
            "content": "Commonsense reasoning and commonsense knowledge in artificial intelligence (Davis &amp; Marcus, 2015). . Summary . This is a very good review paper on commonsense knowledge published on 2015. It was cited by many commonsense related papers. It covers the importance of commonsense knowledge, the objectives, the difficulties, current(by then) approaches and future directions. The illustraing examples are very helpful to enhance readers’ understanding. It is worth special attention to the challenges part. . Commonsense in Intelligent Tasks . NLP, especially reference resolution | CV, such as recognize objects just by context. (recognize a table even we only see the table cloth.) | Robotics, how to make sensible actions | . Successes in Automated Commonsense Reasoning . taxonomic reasoning: (1) can use transitivity of categories and inheritance of properties; (2) not hard for straightforward taxonomy structure, but can be problematic when categories overlap; (3) used in Web mining systems. | temporal reasoning: can be reduced to solving systems of linear inequalities. The main difficulty lies in the interpretation of time (or assigning timestamps to events) in NLP, in that it is context dependent. | Action and change: has applications similar to recipeQA, and limited success in text. | Qualitative reasoning: reason the direction of change in interrelated quantities, ex. pressure will go up if the temperature goes up in a closed container. | . Challenges in Automating Commonsense Reasoning . lack of a complete understanding of some domains, ex. psychology, biology, social instutions. | intuitive situations can be logically complex, ex. understand the behaviour and intention of a character in movies. | plausible reasoning. Commonsense reasoning might be stochastic in nature. | the long-tail issue: some categories show up rarely. the margin profit for building a KB would then be small. | in my opinion, the long-tail issue depends on the abstraction level, which in turn is also difficult to set up consistently in a KB. | . | difficulty to discern the proper level of abstraction | projects that requires longer time to see the payoffs are usually less appealing. (What an insight!) | Objectives of Commonsense Reasoning Research . I pick up the ones that I feel are applicable to NLP here. . Plausible inference | Range of reasoning modes, such as explanation, generalization, abstraction, analogy, and simulation. This can be a direction to work on. | Analysis of fundamental domains, such as intuive physics and intuitive psychology. | Cognitive modeling. Utilize the theories of how human do commonsense reasoning. | . Approaches and Techniques . To build a KB: . mathematically grounded | use KB from informal sources, such as stories and anecdotes | Web mining. The issue is severe confusions and inconsistencies. | Crowd sourcing. Again, inconsistency. | Future Directions . integrate mathematical logic with Web mining. | investigate how facts gathered from Web mining can constrain the development of mathematically grounded theories. | consider other modes of reasoning: analogy, fram-based reasoning, abstraction, conjecture, explanation. | use cognitive science. | Related topics to read . ACL 2020 Commonsense Tutorial (T6) | description logic, an extension of baisc inheritance structure | Deep Learning: A Critical Appraisal | frame-based reasoning | cognitive science, ex. Think: fast and slow. | . References . Davis, E., &amp; Marcus, G. (2015). Commonsense reasoning and commonsense knowledge in artificial intelligence. Communications of the ACM, 58(9), 92–103. |",
            "url": "https://yuyangyy.com/researchlog/nlp/mrc/2021/01/21/commonse-reasoining-in-ai.html",
            "relUrl": "/nlp/mrc/2021/01/21/commonse-reasoining-in-ai.html",
            "date": " • Jan 21, 2021"
        }
        
    
  
    
        ,"post3": {
            "title": "Digest -- Recent advances in natural language inference: A survey of benchmarks, resources, and approaches",
            "content": "Recent advances in natural language inference: A survey of benchmarks, resources, and approaches (Storks et al., 2019). . Summary . This is a really good review paper in NLI. It mainly covers language understanding tasks and benchmarks where we need to use some external knowledge or advanced reasoning beyond linguistic context. The idea that we can better guide researchers to focus on truly understand the reasoning by designing smarter benchmarks is inspiring. This paper gives an overview of existing benchmarks and what problems they are trying to solve, as well as existing knowledge resources and inference approaches. It also provides examples from the benchmark datasets, which can give beginners some basic idea. It can serve as a pretty good reference for resources looking up. Several issues raised in this paper are worth attention, such as the unexplainability of recent approaches and the statistical biases found in benchmark datasets. . Benchmarks and Tasks . Five major tasks require external knowledge and complex reasoning: reference resolution, question answering, textual entailment, plausible inference, and intuitive psychology. It seems to me that the difference between textual entailment and plausible inference is that text entailment judges the correctness of hypothese and focuses on reasoning, while plausible inference finds the event that is the most likely to happen according to commonse knowledge. . The authors also call attention for the superficial correlation biases in the datasets, for example, the gender bias. Mutual information method (Gururangan et al., 2018) and adversarial filtering process (Zellers et al., 2018) may be helpful for such biases. . Knowledge Resources . Linguistic knowledge includes annotated corpora, frame semantics resources, lexical resources, and pre-trained semantic vectors. . Common and commonsense knowledge resources are mostly in the form of knowledge base and knowledge graph. To clarify, common knowledge refers to well-known facts about the world that are often explicitly stated, while commonsense knowledge, on the other hand, is considered obvious to most humans, and not likely to be explicitly stated (Cambria et al., 2011). . Learning and Inference Approaches . Three main neural approaches are brought up: attention mechanism, memory augmentation, and contextual models and representations. . It points out that attention mechanism works well mainly on capturing the alignment between an input and an output, and capturing long-term dependencies. One thing to note is some RNN models with attention will perform worse since there is no such alignment. This reminds us to keep in mind what a structure is actually learning before stacking them altogether. . Memory augmentation methods, such as memory networks, are new to me and requires further reading. . One interesting point about using external knowledge is mentioned: (Mihaylov et al., 2018) find that their adding of facts from ConceptNet causes distraction which reduces performance, suggesting that the technique for selecting the appropriate relations is important to reduce distraction. . Future Directions . The directions are mostly for designing datasets, still, I get some motivations. . Despite the good performance of current models, we don’t know whether or not they are actually performing reasoning. The authors think the benchmarks should differentiate between types of reasoning and take that into evaluations. . A competence-centric evaluation, while important for pushing the state of the art, can also lead to a less productive path if not treated carefully. . The authors suggest that we put more attention on a good understanding of model behaviors (anything insightful? what is the model actually learning?), computational efficiency, and generalization ability (inference on new tasks with minimal training). . Questions . How to use common or commonsense knowledge in creating a benchmark dataset? | What are the existing types of reasoning? | Related topics to read . mutual information | ConceptNet | BERT is indeed learning and exploiting statistical biases in certain benchmarks (Niven &amp; Kao, 2019) . | distributional representation of words (Ferrone &amp; Zanzotto, 2020). | types of reasoning (Davis &amp; Marcus, 2015) | . References . Storks, S., Gao, Q., &amp; Chai, J. Y. (2019). Recent advances in natural language inference: A survey of benchmarks, resources, and approaches. ArXiv Preprint ArXiv:1904.01172. | Gururangan, S., Swayamdipta, S., Levy, O., Schwartz, R., Bowman, S., &amp; Smith, N. A. (2018). Annotation Artifacts in Natural Language Inference Data. Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 2 (Short Papers), 107–112. | Zellers, R., Bisk, Y., Schwartz, R., &amp; Choi, Y. (2018). SWAG: A Large-Scale Adversarial Dataset for Grounded Commonsense Inference. Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, 93–104. | Cambria, E., Song, Y., Wang, H., &amp; Hussain, A. (2011). Isanette: A common and common sense knowledge base for opinion mining. 2011 IEEE 11th International Conference on Data Mining Workshops, 315–322. | Mihaylov, T., Clark, P., Khot, T., &amp; Sabharwal, A. (2018). Can a Suit of Armor Conduct Electricity? A New Dataset for Open Book Question Answering. Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, 2381–2391. | Niven, T., &amp; Kao, H.-Y. (2019). Probing Neural Network Comprehension of Natural Language Arguments. Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, 4658–4664. | Ferrone, L., &amp; Zanzotto, F. M. (2020). Symbolic, distributed, and distributional representations for natural language processing in the era of deep learning: A survey. Frontiers in Robotics and AI, 6, 153. | Davis, E., &amp; Marcus, G. (2015). Commonsense reasoning and commonsense knowledge in artificial intelligence. Communications of the ACM, 58(9), 92–103. |",
            "url": "https://yuyangyy.com/researchlog/nli/2021/01/20/recent-advances-in-nli.html",
            "relUrl": "/nli/2021/01/20/recent-advances-in-nli.html",
            "date": " • Jan 20, 2021"
        }
        
    
  
    
        ,"post4": {
            "title": "Digest -- A Survey on Machine Reading Comprehension: Tasks, Evaluation Metrics, and Benchmark Datasets",
            "content": "A Survey on Machine Reading Comprehension: Tasks, Evaluation Metrics, and Benchmark Datasets (Zeng et al., 2020). . Summary . This paper provides a comprehensive survey on MRC tasks, evaluation metrics, and existing benchmark datasets. I find the Tasks section and the Open Issues section most helpful. . Tasks . a definition of typical MRC tasks is given, which can be seen as a supervised problem: (context, question -&gt; answer). | concept clarification about MRC multi-modal MRC vs. textual MRC: multi-modal MRC also involves images and videos, such as RecipeQA and MovieQA. | MRC vs. QA: These two tasks are not subsets of one another. | Some MRC may be seen as a special case QA, in that QA can also be open-domain and that QA can also be solved by rule-based method, information retrival method and knowledge-based method. | On the other hand, just like human, reading comhension can be about giving correct answers to questions, and can also be about asking the right or sensible questions given the context. And in multi-modal MRC, QA is just one part of it, and we also need CV. | . | MRC vs. NLP. Syntax information can help with MT, and some MRC models can be used in NLI as well (ex. SAN). (Need to figure out the definitions of NLP and NLI.) | . | Classification of MRC Tasks (clear and well-defined) type of corpus: multi-modal, textual | type of questions: cloze style, natural, synthetic | type of answers: natural, multiple choice | source of answers: span, free-form | . | Benchmark Datasets . In the Benchmark Dataset section, the authors list almost all available datasets and they kindly provide a website summarizing all the datasets. One good feature I like the most is the prerequisite skills (Table 8) and an overview of the characterisitcs of each dataset (Table 10). The prerequisite skills may provide some ideas on building new models and interpretation. And among the characteristics, I am the most interested in Complex Reasoning. . I checked most of them and found that some of them were not active in these two years. I hereby list the ones that I find active and interesting and also with leaderboard. I care about leaderboard is because I want to check the gap between the state-of-art of human performance to see further improvement potential. . ARC: commonsense knowledge and complex reasoning | OpenBookQA: commonsense knowledge | ReCoRD (part of SuperGLUE now): commonsense knowledge | HotpotQA | SciTail | DROP: complex reasonsing | RACE: passage reading comprehension from middle- and high-school English exams. Involve complex reasoning. I am intersted in this dataset since the questions in the exams are usually made up by experts and should have higher quality. | TriviaQA | SQuAD | CoQA: conversational QA | SuperGLUE | . In these leaderboards, UnifiedQA (Khashabi et al., 2020), XLNet (Yang et al., 2019), ALBERT (Lan et al., 2019) , RoBERTa (Liu et al., 2019) , T5 (Raffel et al., 2019) , and DeBERTa (He et al., 2020) are models that achieve good results. . Open Issues . In the Open Issues section, the authors think multi-modal MRC, commonsense knowledge, complex reasoning, robustness, and interpretability is worth investigation. . Related topics to read next . UnifiedQA (Khashabi et al., 2020) | XLNet (Yang et al., 2019) | ALBERT (Lan et al., 2019) | . References . Zeng, C., Li, S., Li, Q., Hu, J., &amp; Hu, J. (2020). A Survey on Machine Reading Comprehension—Tasks, Evaluation Metrics and Benchmark Datasets. Applied Sciences, 10(21), 7640. | Khashabi, D., Khot, T., Sabharwal, A., Tafjord, O., Clark, P., &amp; Hajishirzi, H. (2020). Unifiedqa: Crossing format boundaries with a single qa system. ArXiv Preprint ArXiv:2005.00700. | Yang, Z., Dai, Z., Yang, Y., Carbonell, J., Salakhutdinov, R. R., &amp; Le, Q. V. (2019). Xlnet: Generalized autoregressive pretraining for language understanding. Advances in Neural Information Processing Systems, 5753–5763. | Lan, Z., Chen, M., Goodman, S., Gimpel, K., Sharma, P., &amp; Soricut, R. (2019). Albert: A lite bert for self-supervised learning of language representations. ArXiv Preprint ArXiv:1909.11942. | Liu, Y., Ott, M., Goyal, N., Du, J., Joshi, M., Chen, D., Levy, O., Lewis, M., Zettlemoyer, L., &amp; Stoyanov, V. (2019). Roberta: A robustly optimized bert pretraining approach. ArXiv Preprint ArXiv:1907.11692. | Raffel, C., Shazeer, N., Roberts, A., Lee, K., Narang, S., Matena, M., Zhou, Y., Li, W., &amp; Liu, P. J. (2019). Exploring the limits of transfer learning with a unified text-to-text transformer. ArXiv Preprint ArXiv:1910.10683. | He, P., Liu, X., Gao, J., &amp; Chen, W. (2020). DeBERTa: Decoding-enhanced BERT with Disentangled Attention. ArXiv Preprint ArXiv:2006.03654. |",
            "url": "https://yuyangyy.com/researchlog/nlp/mrc/2021/01/19/a-survey-on-mrc.html",
            "relUrl": "/nlp/mrc/2021/01/19/a-survey-on-mrc.html",
            "date": " • Jan 19, 2021"
        }
        
    
  
    
        ,"post5": {
            "title": "Fastpages Notebook Blog Post",
            "content": "About . This notebook is a demonstration of some of capabilities of fastpages with notebooks. . With fastpages you can save your jupyter notebooks into the _notebooks folder at the root of your repository, and they will be automatically be converted to Jekyll compliant blog posts! . Front Matter . The first cell in your Jupyter Notebook or markdown blog post contains front matter. Front matter is metadata that can turn on/off options in your Notebook. It is formatted like this: . # &quot;My Title&quot; &gt; &quot;Awesome summary&quot; - toc:true- branch: master - badges: true - comments: true - author: Hamel Husain &amp; Jeremy Howard - categories: [fastpages, jupyter] . Setting toc: true will automatically generate a table of contents | Setting badges: true will automatically include GitHub and Google Colab links to your notebook. | Setting comments: true will enable commenting on your blog post, powered by utterances. | . The title and description need to be enclosed in double quotes only if they include special characters such as a colon. More details and options for front matter can be viewed on the front matter section of the README. . Markdown Shortcuts . A #hide comment at the top of any code cell will hide both the input and output of that cell in your blog post. . A #hide_input comment at the top of any code cell will only hide the input of that cell. . The comment #hide_input was used to hide the code that produced this. . put a #collapse-hide flag at the top of any cell if you want to hide that cell by default, but give the reader the option to show it: . import pandas as pd import altair as alt . . put a #collapse-show flag at the top of any cell if you want to show that cell by default, but give the reader the option to hide it: . cars = &#39;https://vega.github.io/vega-datasets/data/cars.json&#39; movies = &#39;https://vega.github.io/vega-datasets/data/movies.json&#39; sp500 = &#39;https://vega.github.io/vega-datasets/data/sp500.csv&#39; stocks = &#39;https://vega.github.io/vega-datasets/data/stocks.csv&#39; flights = &#39;https://vega.github.io/vega-datasets/data/flights-5k.json&#39; . . place a #collapse-output flag at the top of any cell if you want to put the output under a collapsable element that is closed by default, but give the reader the option to open it: . print(&#39;The comment #collapse-output was used to collapse the output of this cell by default but you can expand it.&#39;) . The comment #collapse-output was used to collapse the output of this cell by default but you can expand it. . . Interactive Charts With Altair . Charts made with Altair remain interactive. Example charts taken from this repo, specifically this notebook. . Example 1: DropDown . # use specific hard-wired values as the initial selected values selection = alt.selection_single( name=&#39;Select&#39;, fields=[&#39;Major_Genre&#39;, &#39;MPAA_Rating&#39;], init={&#39;Major_Genre&#39;: &#39;Drama&#39;, &#39;MPAA_Rating&#39;: &#39;R&#39;}, bind={&#39;Major_Genre&#39;: alt.binding_select(options=genres), &#39;MPAA_Rating&#39;: alt.binding_radio(options=mpaa)} ) # scatter plot, modify opacity based on selection alt.Chart(df).mark_circle().add_selection( selection ).encode( x=&#39;Rotten_Tomatoes_Rating:Q&#39;, y=&#39;IMDB_Rating:Q&#39;, tooltip=&#39;Title:N&#39;, opacity=alt.condition(selection, alt.value(0.75), alt.value(0.05)) ) . Example 2: Tooltips . alt.Chart(df).mark_circle().add_selection( alt.selection_interval(bind=&#39;scales&#39;, encodings=[&#39;x&#39;]) ).encode( alt.X(&#39;Rotten_Tomatoes_Rating&#39;, type=&#39;quantitative&#39;), alt.Y(&#39;IMDB_Rating&#39;, type=&#39;quantitative&#39;, axis=alt.Axis(minExtent=30)), # y=alt.Y(&#39;IMDB_Rating:Q&#39;, ), # use min extent to stabilize axis title placement tooltip=[&#39;Title:N&#39;, &#39;Release_Date:N&#39;, &#39;IMDB_Rating:Q&#39;, &#39;Rotten_Tomatoes_Rating:Q&#39;] ).properties( width=500, height=400 ) . Example 3: More Tooltips . label = alt.selection_single( encodings=[&#39;x&#39;], # limit selection to x-axis value on=&#39;mouseover&#39;, # select on mouseover events nearest=True, # select data point nearest the cursor empty=&#39;none&#39; # empty selection includes no data points ) # define our base line chart of stock prices base = alt.Chart().mark_line().encode( alt.X(&#39;date:T&#39;), alt.Y(&#39;price:Q&#39;, scale=alt.Scale(type=&#39;log&#39;)), alt.Color(&#39;symbol:N&#39;) ) alt.layer( base, # base line chart # add a rule mark to serve as a guide line alt.Chart().mark_rule(color=&#39;#aaa&#39;).encode( x=&#39;date:T&#39; ).transform_filter(label), # add circle marks for selected time points, hide unselected points base.mark_circle().encode( opacity=alt.condition(label, alt.value(1), alt.value(0)) ).add_selection(label), # add white stroked text to provide a legible background for labels base.mark_text(align=&#39;left&#39;, dx=5, dy=-5, stroke=&#39;white&#39;, strokeWidth=2).encode( text=&#39;price:Q&#39; ).transform_filter(label), # add text labels for stock prices base.mark_text(align=&#39;left&#39;, dx=5, dy=-5).encode( text=&#39;price:Q&#39; ).transform_filter(label), data=stocks ).properties( width=500, height=400 ) . Data Tables . You can display tables per the usual way in your blog: . df[[&#39;Title&#39;, &#39;Worldwide_Gross&#39;, &#39;Production_Budget&#39;, &#39;Distributor&#39;, &#39;MPAA_Rating&#39;, &#39;IMDB_Rating&#39;, &#39;Rotten_Tomatoes_Rating&#39;]].head() . Title Worldwide_Gross Production_Budget Distributor MPAA_Rating IMDB_Rating Rotten_Tomatoes_Rating . 0 The Land Girls | 146083.0 | 8000000.0 | Gramercy | R | 6.1 | NaN | . 1 First Love, Last Rites | 10876.0 | 300000.0 | Strand | R | 6.9 | NaN | . 2 I Married a Strange Person | 203134.0 | 250000.0 | Lionsgate | None | 6.8 | NaN | . 3 Let&#39;s Talk About Sex | 373615.0 | 300000.0 | Fine Line | None | NaN | 13.0 | . 4 Slam | 1087521.0 | 1000000.0 | Trimark | R | 3.4 | 62.0 | . Images . Local Images . You can reference local images and they will be copied and rendered on your blog automatically. You can include these with the following markdown syntax: . ![](my_icons/fastai_logo.png) . . Remote Images . Remote images can be included with the following markdown syntax: . ![](https://image.flaticon.com/icons/svg/36/36686.svg) . . Animated Gifs . Animated Gifs work, too! . ![](https://upload.wikimedia.org/wikipedia/commons/7/71/ChessPawnSpecialMoves.gif) . . Captions . You can include captions with markdown images like this: . ![](https://www.fast.ai/images/fastai_paper/show_batch.png &quot;Credit: https://www.fast.ai/2020/02/13/fastai-A-Layered-API-for-Deep-Learning/&quot;) . . Other Elements . GitHub Flavored Emojis . Typing I give this post two :+1:! will render this: . I give this post two :+1:! . Tweetcards . Typing &gt; twitter: https://twitter.com/jakevdp/status/1204765621767901185?s=20 will render this: Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 . Youtube Videos . Typing &gt; youtube: https://youtu.be/XfoYk_Z5AkI will render this: . Boxes / Callouts . Typing &gt; Warning: There will be no second warning! will render this: . Warning: There will be no second warning! . Typing &gt; Important: Pay attention! It&#39;s important. will render this: . Important: Pay attention! It&#8217;s important. . Typing &gt; Tip: This is my tip. will render this: . Tip: This is my tip. . Typing &gt; Note: Take note of this. will render this: . Note: Take note of this. . Typing &gt; Note: A doc link to [an example website: fast.ai](https://www.fast.ai/) should also work fine. will render in the docs: . Note: A doc link to an example website: fast.ai should also work fine. . Footnotes . You can have footnotes in notebooks, however the syntax is different compared to markdown documents. This guide provides more detail about this syntax, which looks like this: . For example, here is a footnote {% fn 1 %}. And another {% fn 2 %} {{ &#39;This is the footnote.&#39; | fndetail: 1 }} {{ &#39;This is the other footnote. You can even have a [link](www.github.com)!&#39; | fndetail: 2 }} . For example, here is a footnote 1. . And another 2 . 1. This is the footnote.↩ . 2. This is the other footnote. You can even have a link!↩ .",
            "url": "https://yuyangyy.com/researchlog/jupyter/2020/02/20/test.html",
            "relUrl": "/jupyter/2020/02/20/test.html",
            "date": " • Feb 20, 2020"
        }
        
    
  
    
        ,"post6": {
            "title": "An Example Markdown Post",
            "content": "Example Markdown Post . Basic setup . Jekyll requires blog post files to be named according to the following format: . YEAR-MONTH-DAY-filename.md . Where YEAR is a four-digit number, MONTH and DAY are both two-digit numbers, and filename is whatever file name you choose, to remind yourself what this post is about. .md is the file extension for markdown files. . The first line of the file should start with a single hash character, then a space, then your title. This is how you create a “level 1 heading” in markdown. Then you can create level 2, 3, etc headings as you wish but repeating the hash character, such as you see in the line ## File names above. . Basic formatting . You can use italics, bold, code font text, and create links. Here’s a footnote 1. Here’s a horizontal rule: . . Lists . Here’s a list: . item 1 | item 2 | . And a numbered list: . item 1 | item 2 | Boxes and stuff . This is a quotation . . You can include alert boxes …and… . . You can include info boxes Images . . Code . You can format text and code per usual . General preformatted text: . # Do a thing do_thing() . Python code and output: . # Prints &#39;2&#39; print(1+1) . 2 . Formatting text as shell commands: . echo &quot;hello world&quot; ./some_script.sh --option &quot;value&quot; wget https://example.com/cat_photo1.png . Formatting text as YAML: . key: value - another_key: &quot;another value&quot; . Tables . Column 1 Column 2 . A thing | Another thing | . Tweetcards . Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 Footnotes . This is the footnote. &#8617; . |",
            "url": "https://yuyangyy.com/researchlog/markdown/2020/01/14/test-markdown-post.html",
            "relUrl": "/markdown/2020/01/14/test-markdown-post.html",
            "date": " • Jan 14, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "Hi, I am Yu Yang, a third-year Ph.D. student in Statistics. I am interested in Natural Language Processing and Machine Learning. . You may also visit my Homepage and Personal Blog. . Happy reading! .",
          "url": "https://yuyangyy.com/researchlog/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
      ,"page9": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://yuyangyy.com/researchlog/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

  
  

}