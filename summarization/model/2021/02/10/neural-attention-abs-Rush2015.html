<!DOCTYPE html>
<html lang="en"><head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="twitter:card" content="summary_large_image" /><!-- Begin Jekyll SEO tag v2.6.1 -->
<title>Digest – A Neural Attention Model for Abstractive Sentence Summarization Alexander | Yu Yang’s Research Log</title>
<meta name="generator" content="Jekyll v4.1.1" />
<meta property="og:title" content="Digest – A Neural Attention Model for Abstractive Sentence Summarization Alexander" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="A Neural Attention Model for Abstractive Sentence Summarization Alexander (Rush et al., 2015)." />
<meta property="og:description" content="A Neural Attention Model for Abstractive Sentence Summarization Alexander (Rush et al., 2015)." />
<link rel="canonical" href="https://yuyangyy.com/researchlog/summarization/model/2021/02/10/neural-attention-abs-Rush2015.html" />
<meta property="og:url" content="https://yuyangyy.com/researchlog/summarization/model/2021/02/10/neural-attention-abs-Rush2015.html" />
<meta property="og:site_name" content="Yu Yang’s Research Log" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2021-02-10T20:26:00-06:00" />
<script type="application/ld+json">
{"url":"https://yuyangyy.com/researchlog/summarization/model/2021/02/10/neural-attention-abs-Rush2015.html","@type":"BlogPosting","headline":"Digest – A Neural Attention Model for Abstractive Sentence Summarization Alexander","dateModified":"2021-02-10T20:26:00-06:00","datePublished":"2021-02-10T20:26:00-06:00","mainEntityOfPage":{"@type":"WebPage","@id":"https://yuyangyy.com/researchlog/summarization/model/2021/02/10/neural-attention-abs-Rush2015.html"},"description":"A Neural Attention Model for Abstractive Sentence Summarization Alexander (Rush et al., 2015).","@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->
<link rel="stylesheet" href="/researchlog/assets/css/style.css"><link type="application/atom+xml" rel="alternate" href="https://yuyangyy.com/researchlog/feed.xml" title="Yu Yang's Research Log" /><link rel="shortcut icon" type="image/x-icon" href="/researchlog/images/favicon.ico"><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/Primer/15.2.0/primer.css" integrity="sha512-xTz2ys4coGAOz8vuV1NcQBkgVmKhsSEtjbqyMJbBHRplFuvKIUo6xhLHpAyPt9mfR6twHJgn9OgVLuqOvjeBhg==" crossorigin="anonymous" />
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.14.0/css/all.min.css" integrity="sha512-1PKOgIY59xJ8Co8+NE6FZ+LOAZKjy+KY8iq0G4B3CyeY6wYHN3yt9PW0XpSriVlkMXe40PTKnXrLnZ9+fkDaog==" crossorigin="anonymous" />

<script>
function wrap_img(fn) {
    if (document.attachEvent ? document.readyState === "complete" : document.readyState !== "loading") {
        var elements = document.querySelectorAll(".post img");
        Array.prototype.forEach.call(elements, function(el, i) {
            if (el.getAttribute("title") && (el.className != "emoji")) {
                const caption = document.createElement('figcaption');
                var node = document.createTextNode(el.getAttribute("title"));
                caption.appendChild(node);
                const wrapper = document.createElement('figure');
                wrapper.className = 'image';
                el.parentNode.insertBefore(wrapper, el);
                el.parentNode.removeChild(el);
                wrapper.appendChild(el);
                wrapper.appendChild(caption);
            }
        });
    } else { document.addEventListener('DOMContentLoaded', fn); }
}
window.onload = wrap_img;
</script>

<script>
    document.addEventListener("DOMContentLoaded", function(){
    // add link icon to anchor tags
    var elem = document.querySelectorAll(".anchor-link")
    elem.forEach(e => (e.innerHTML = '<i class="fas fa-link fa-xs"></i>'));
    });
</script>
</head>
<body><header class="site-header">

  <div class="wrapper"><a class="site-title" rel="author" href="/researchlog/">Yu Yang&#39;s Research Log</a><nav class="site-nav">
        <input type="checkbox" id="nav-trigger" class="nav-trigger" />
        <label for="nav-trigger">
          <span class="menu-icon">
            <svg viewBox="0 0 18 15" width="18px" height="15px">
              <path d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.032C17.335,0,18,0.665,18,1.484L18,1.484z M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.032C17.335,6.031,18,6.696,18,7.516L18,7.516z M18,13.516C18,14.335,17.335,15,16.516,15H1.484 C0.665,15,0,14.335,0,13.516l0,0c0-0.82,0.665-1.483,1.484-1.483h15.032C17.335,12.031,18,12.695,18,13.516L18,13.516z"/>
            </svg>
          </span>
        </label>

        <div class="trigger"><a class="page-link" href="/researchlog/about/">About Me</a><a class="page-link" href="/researchlog/search/">Search</a><a class="page-link" href="/researchlog/categories/">Tags</a></div>
      </nav></div>
</header>
<main class="page-content" aria-label="Content">
      <div class="wrapper">
        <article class="post h-entry" itemscope itemtype="http://schema.org/BlogPosting">

  <header class="post-header">
    <h1 class="post-title p-name" itemprop="name headline">Digest -- A Neural Attention Model for Abstractive Sentence Summarization Alexander</h1><p class="post-meta post-meta-title"><time class="dt-published" datetime="2021-02-10T20:26:00-06:00" itemprop="datePublished">
        Feb 10, 2021
      </time>
       • <span class="read-time" title="Estimated read time">
    
    
      1 min read
    
</span></p>

    
      <p class="category-tags"><i class="fas fa-tags category-tags-icon"></i></i> 
      
        <a class="category-tags-link" href="/researchlog/categories/#Summarization">Summarization</a>
        &nbsp;
      
        <a class="category-tags-link" href="/researchlog/categories/#Model">Model</a>
        
      
      </p>
    

    </header>

  <div class="post-content e-content" itemprop="articleBody">
    <p><a href="https://arxiv.org/abs/1509.00685">A Neural Attention Model for Abstractive Sentence Summarization Alexander</a>  <a class="citation" href="#rush2015neural">(Rush et al., 2015)</a>.</p>

<h2 id="summary">Summary</h2>
<p>This is a highly cited paper in abstractive summarization. It might be the first paper to use neural attention in the encoder of the ABS system. The architecture of the model may not look fancy nowadays, but the key concepts and definitions in this paper are well worth noting. The definitions give a high level about the problem set up, and I really recommend a closer look at the Background section and the Encoders subsection.</p>

<h3 id="problem-setup">Problem Setup</h3>
<p>One highlight is that this paper defines the problem using a scoring function $s$, which is a more general concept than conditional probabilities. Currently, most papers focus on optimizing the conditional probability, while at the same time, it might be worthwhile to think about other sensible score functions.</p>

<p>Let x be the original text and y be the summary. An abstractive system optimize (1) while an extractive system works on (2), and a sentence compression system works on (3).</p>

<p><img src="/researchlog/assets/img/posts/20210210-Rush-eq1.png" alt="Rush-eq1" /></p>

<p><img src="/researchlog/assets/img/posts/20210210-Rush-eq2.png" alt="Rush-eq2" /></p>

<p><img src="/researchlog/assets/img/posts/20210210-Rush-eq3.png" alt="Rush-eq3" /></p>

<p>A few personal thoughts:</p>
<ul>
  <li>Extractive summarization is in general easier than abstractive summarization. It is probably due to the constraint imposed in (2). Is it possible to simplify the abstraticve system by adding some sensible constraints?</li>
</ul>

<p>Another highlight is that they consider a special case of scoring function, which is still more general than conditional probabilities, namely, the factored scoring functions.</p>

<p><img src="/researchlog/assets/img/posts/20210210-Rush-eq4.png" alt="Rush-eq4" /></p>

<table>
  <tbody>
    <tr>
      <td>In terms of conditional probability, $s(x, y) = \log p(y</td>
      <td>x;\theta)$ and</td>
    </tr>
  </tbody>
</table>

<p><img src="/researchlog/assets/img/posts/20210210-Rush-condprob.png" alt="Rush-condprob" /></p>

<h3 id="model">Model</h3>
<p>This paper compares three encoders: Bag-of_Words Encoder, Convolutional Encoder, and Attention-Based Encoder.</p>
<ul>
  <li>Bag-of_Words Encoder: can capture the relative importance of words to distinguish con- tent words from stop words or embellishments.</li>
  <li>Convolutional Encoder: improves on the bag-of-words model by allowing local interactions.</li>
  <li>Attention-Based Encoder: can informally think of it as replacing the uniform distribution in bag-of-words with a learned soft alignment between the input and the summary.</li>
</ul>

<h2 id="references">References</h2>

<!-- <a class="citation" href="#"></a> -->

<ol class="bibliography"><li><span id="rush2015neural">Rush, A. M., Chopra, S., &amp; Weston, J. (2015). A Neural Attention Model for Abstractive Sentence Summarization. <i>Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing</i>, 379–389.</span></li></ol>

  </div><!-- from https://github.com/utterance/utterances -->
<script src="https://utteranc.es/client.js"
        repo="yuyangstatistics/researchlog"
        issue-term="title"
        label="blogpost-comment"
        theme="github-light"
        crossorigin="anonymous"
        async>
</script><a class="u-url" href="/researchlog/summarization/model/2021/02/10/neural-attention-abs-Rush2015.html" hidden></a>
</article>
      </div>
    </main><footer class="site-footer h-card">
  <data class="u-url" href="/researchlog/"></data>

  <div class="wrapper">

    <div class="footer-col-wrapper">
      <div class="footer-col">
        <p class="feed-subscribe">
          <a href="/researchlog/feed.xml">
            <svg class="svg-icon orange">
              <use xlink:href="/researchlog/assets/minima-social-icons.svg#rss"></use>
            </svg><span>Subscribe</span>
          </a>
        </p>
      </div>
      <div class="footer-col">
        <p>Statistics Ph.D. student/ Yu Yang. My blog about paper reading, research ideas, and code.</p>
      </div>
    </div>

    <div class="social-links"><ul class="social-media-list"><li><a rel="me" href="https://github.com/yuyangstatistics" title="yuyangstatistics"><svg class="svg-icon grey"><use xlink:href="/researchlog/assets/minima-social-icons.svg#github"></use></svg></a></li><li><a rel="me" href="https://twitter.com/yuyangstat" title="yuyangstat"><svg class="svg-icon grey"><use xlink:href="/researchlog/assets/minima-social-icons.svg#twitter"></use></svg></a></li></ul>
</div>

  </div>

</footer>
</body>

</html>
